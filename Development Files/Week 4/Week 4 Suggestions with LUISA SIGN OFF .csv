Title,Timestamp,Original video Transcript,Summary & Key Points,Suggestions,Visuals,,,,,,,,,,,,,,,,,,,
No Ttitle,0 - 0:39,"What is generative AI? This is week four
of the course. Here are the learning
outcomes. Explain what generative AI is.
List some natural language processing
tasks.
Understand what large language models
and foundational models are. Apply
prompt engineering.
Explain different kinds of
customizations such as fine-tuning and
retrieval augmented generation or RAG.
Discuss human feedback.
Outline the generative AI life cycle.
Explain AI agents and agentic AI. Design
prototypes for NLP tasks. ",the summary of this week's course,,,,,,,,,,,,,,,,,,,,,
Generative AI,0:40-3:18,"We start by
talking about what generative AI is.
Everybody has experience with generative
AI at some point, usually with chat bots
or generating images from text or using
a co-pilot to help develop code.
Generative AI is a subpart of machine
learning that creates content. These AIs
are capable of creating content which is
traditionally a human ability. That's
why the capabilities of these models are
impressive. They can mimic human
creativity. Generative AI is a subset of
traditional machine learning. These are
machines learning to accomplish tasks
which can include generating video,
images, or text. These models are
trained on massive data sets of content
originally generated by humans. For
generative AI that tackles text or
natural language, the models are trained
on billions or trillions of words over a
large period of time, usually 5 to 6
months. Using large compute power, they
can train on this vast corpus of text,
including web pages, PDF files, books,
press publications, and more. Generative
AI can be related to language,
generating text or audio, image, and
video. In this week, we are
focusing on generative AI that generates
text. The underlying capability is text
generation, and it is used to accomplish
several other tasks. Tasks include
answering questions, summarizing texts
and translations,
everything that involves generating
words. These models are called large
language models, LLMs. They are trained
over large corpora of text. The most
common large language models are known
as foundation models. Chat GPT, for
example, is an interface that lets users
access a model which is part of the GPT
family. Other foundation models include
Gemini from Google, Claude from
Anthropic, Llama from Meta, and Grock
from XAI. These are called foundation
models because they unlock the ability
to generate text and perform complex
tasks. They are large because they are
trained on a vast amount of data and
contain billions or trillions of
parameters. A parameter is a knob that
gets adjusted during training so the
model can accomplish a task. There is a
difference between classical machine
learning models like those used for
classification, regression or
unsupervised learning and large language
models.
", what generative AI is,"1:28 It says: ""in this first week we are focusing on ..."", while it is fourth week. Corrected

2:40 the definition of LLMs is confusing. It isn't clear what is the difference between LLMs and GenAIs.  SO, in the text is very well defined but I think it could have gotten complicated because the definitions are inside the same slide. So Edtech people could you please split the generative AI slide into 2? One with the items up until large language models and then the other slides with the rest of the LLM related items?",,,,,,,,,,,,,,,,,,,,
  No title,3:19-4:54,"The way you interact with them
is different. In traditional machine
learning, you program using logic and
code to train a model. Large language
models on the other hand accept natural
language directly from humans.
You don't have to programmatically
specify the task. You can just instruct
the model using natural language. The
text passed to a language model is
called a prompt. And the space available
to write the prompt is called the
context. The context window can span
thousands of words, often one or two
pages depending on the model. Unlike
traditional models that you can easily
train with tools like Python or noode
platforms like Nime or Weta, large
language models take months to train.
These foundation models are not
retrained from scratch every day.
Instead, they are improved using
techniques like fine-tuning or retrieval
augmented generation which will be
covered later in the course. It is
possible to train a large language model
from scratch for a specific task, but it
requires substantial computational
resources and large data sets. That's
why these pre-trained models are called
foundation models and people use
engineering techniques to adapt them.
Some companies do train their own LLMs.
Examples of LLM's GPT, Open AI, BERT,
Google, LA, Meta, Flan, open source,
Palm and Bloom, open source",the difference between LLM and traditional machine learning; the way LLM is trained; some examples of LLM,,,,,,,,,,,,,,,,,,,,,
LLM capabilities,4:55 - 6:04,"NLP is the field that handles human
language using computational methods.
Traditional NLP tasks include
translation, summarization, and
generation.",capabilities of LLM,,,,,,,,,,,,,,,,,,,,,
computatio0nally expensive,6:05 - 6:41,".
Natural language processing
before LLM's natural language processing
NLP is the field that handles human
language using computational methods.
Traditional NLP tasks include
translation, summarization, and
generation."," the number of paprameters needed for building GPT 3, GPT 3.5, and GPT4.",6:07 the points mentioned about the number of parameters in GPTs are detached from other points. There needs to be something to connect it. Maybe it's better to bring this section when it is saying that it takes months to train these models. SOLVED in the script,,,,,,,,,,,,,,,,,,,,
No Ttitle,6:42-7:37,"Before large language models, these
tasks were handled by recurrent neural
networks, RNNs. An RNN would only retain
the last few words like my tea tasted
and might complete the sentence with
great, ignoring that the earlier part
said the milk was bad. Because of
limited context, RNN's couldn't
effectively handle word completion
tasks.
Another difficult task was
summarization. Example, I traveled to
fish at a river,
but before going, I got some money from
the bank. It's unclear whether bank
refers to a financial institution or a
river bank. RNN's had difficulty with
this type of ambiguity due to their
limited context.",the limitation of RNN,,,,,,,,,,,,,,,,,,,,,
How LLMs work,7:38 - 9:37,"Then came the
transformer architecture. A 2019 paper
titled Attention Is All You Need from
Google Deep Mind and the University of
Toronto introduced the transformer, a
deep learning architecture for NLP.
Transformers are based on deep neural
networks, but architected to allow
parallel processing of input tokens.
RNNs process one word at a time, but
transformers can process all words in a
sentence simultaneously. Example with
RNN's the word 'the' is processed
through the entire network then milk
then is and so on one at a time.
With transformers all words like
elephants are the are processed
together.
This allows the model to predict the
next word based on the entire sentence
at once.
Because of this parallelization,
transformers allow for much longer
context windows and better performance.
This architecture also takes advantage
of GPUs, graphical processing units,
which can process millions of elements
simultaneously.
LLMs are massive probabilistic systems
predicting the next token in a sequence.
Example, elephants are the possible
completions include largest, smartest,
gentlest, most majestic, most
impressive.
The choice depends on the model's
training data. If largest is the word
with the highest probability, the model
might select it. But to introduce
creativity and variability, a bit of
randomness is added. That's why the same
prompt often gives different responses.
Each time you ask a question or request
a summary from chat GPT, for example,
you may get a different answer.
This built-in randomness makes
generative AI more dynamic and creative.",How the transformer architecture and GPU help LLM overcome the limitations that RNN has,It was great the transition from the RNN architecture to transformers (8:00),,,,,,,,,,,,,,,,,,,,
generative configuration - inference parameters,9:38 - 13:25,"We will talk about some parameters found
in large language models. When using a
tool like chat GPT, you can adjust
several parameters that control how the
model performs inference, the process of
generating outputs after training. These
are known as inference parameters. There
are four main inference parameters.
One, max new tokens. This sets the
maximum number of new tokens the model
can generate in a response. Tokens
represent segments of text, often whole
words or word pieces, depending on the
tokenizer. If you set this value to a
smaller number, the model will generate
shorter responses. This is a cap, not a
fixed length. The model can generate up
to this number of tokens.
Two, topase sampling. The model produces
a probability distribution over all
known words or tokens in its vocabulary
for the next token prediction. For
example, it might calculate the highest
probabilities as cake, donut, banana,
apple. If the model simply chooses the
highest probability every time, it would
always return the same answer, leading
to repetitive behavior.
Top case sampling introduces randomness.
It limits the model to only consider the
top k most probable tokens and then
selects randomly among them. Example, if
cake, donut, and banana are the top
three options, top k sampling with kgle
three, we'll randomly pick one of those
three. This makes the output more
varied. The larger the value of K, the
broader the set of words from which the
model can choose and the more diverse
the outputs will be. A small K makes the
output more deterministic. A large K
increases randomness and variety.
Top P sampling takes a different
approach. Rather than selecting a fixed
number of top candidates, it selects
tokens from the top ranked options until
their cumulative probability reaches a
threshold point. If cake equals 0.18,
donut equals 0.11, banana equals 0.06,
apple equals 0.05, then a cumulative.3
might include just cake and donut. The
model will randomly select between
those.
Top P makes the selection dynamic. It
always uses the smallest set of top
tokens whose cumulative probability
reaches P. This means it can adapt to
the probability distribution of the
model's output for that context. Top P
leads to randomness like top K, but is
more probability aware and tends to stay
within more plausible answers.
In contrast, top K may include low
probability options if K is set high,
making the model more creative but
potentially less coherent.
Temperature controls how peaked or flat
the probability distribution is before
sampling. It is a scaling factor applied
to the probability values. A low
temperature, eg 0.7, sharpens the
distribution, making high probability
tokens even more dominant. This makes
the model more deterministic. A high
temperature, eg 1.5,
flattens the distribution, making lower
probability tokens more likely to be
selected. This makes the model more
creative.
In a visual analogy, a low temperature
yields a histogram where cake has a very
high bar and the others have short bars.
A high temperature yields a more even
histogram. Cake, donut, banana, and
apple are more equally likely. If you
apply top P or top K sampling after
temperature adjustment, the diversity of
your outputs increases with higher
temperature.","we can adjust parameters to control how the LLM performs. Such parameters in ChatGPT include max new tokens, sample top k, sample top p, temperature.",,,,,,,,,,,,,,,,,,,,,
self attention,13:26 - 14:49,"The transformer
architecture and the innovation behind
LLMs.
This is not meant to be an exhaustive
explanation of how large language models
work. That would take an entire course.
The goal is to understand how the models
work conceptually so we can better
understand how to fine-tune them and
compare foundational models.
Understanding context through self-
attention.
The most important concept behind large
language models is how they learn from
and make sense of context. The model
excels at understanding context and the
mechanism that enables this is called
self attention.
This is why the foundational paper from
Google in 2017 is titled attention is
all you need. In self- attention, the
model computes the relevance of each
word to every other word in a sentence.
This can be visualized as an attention
map. Example sentence, the teacher
taught the student with a book.
In this case, the word teacher is
strongly connected to taught and book,
slightly less to student, and minimally
to the article a. The attention
mechanism allows the model to learn
these relationships and use them to
understand context. For example,
determining who has the book even when
it's ambiguous.",how LLMs understand context through self-attention,"13:40 this section that is explained here, is better to come earlier, in the beginning of talking about LLMs. ""This is not meant to be an exhaustive explanation of how large language models work. That would take an entire course. The goal is to understand how the models work conceptually so we can better understand how to fine-tune them and compare foundational models."" I dont hink so, it causes students to drift because it is more mathematical. So it is best to keep exaples first and then have this quick theory after",,,,,,,,,,,,,,,,,,,,
transformers,14:50 - 16:28,"The trans the
transformer architecture consists of two
main components encoder decoder.
Together, these process the input and
generate a probability distribution over
all the words in the model's vocabulary.
The model is a statistical calculator
that needs to transform words into
numbers. This begins with tokenization.
Tokenization converts each word into an
integer ID, the 342, teacher 879, etc.
These token IDs are then passed into an
embedding layer which maps each token
into a vector in a multi-dimensional
space.
Similar words semantically and
grammatically end up close together. For
example, in a 3D embedding space, cat
and dog are close. Helicopter is far
away. Unfaithful might also be close to
dog because of language usage. Eg. He's
a dog.
This reflects self-supervised learning.
The model constructs an internal latent
space through training. The dimensions
are not humanlabeled, but the model
learns to position words based on usage
patterns.
This embedding process was discussed
earlier during the topic of recommener
systems. Positional
encoding
transformers unlike RNN's process all
words in parallel rather than
sequentially. This makes it necessary to
indicate position explicitly. This is
done through positional encoding which
assigns a vector to each token to
indicate its order. For example, cats
are friends. Cats position one. R
position two, friends position three.",how the transformer architecture work,"Abrupt interruption (14:50)
Show only one image while explaining (14:55)
Resize the slide to fit better (15:00) - The interruption is just a glitch in the video. ",,,,,,,,,,,,,,,,,,,,
self attention,16:29 - 18:29,"Both the encoder and the decoder apply
can apply the uh self attention
mechanism and they apply several times
um which is called a multi head
attention. So after tokenizing creating
ids integer ids transforming these ids
to vectors and then
putting a mechanism to keep this the
position of the words of the tokens
although this is processed in parallel
you can you will keep that information
the encoder and decoder will apply self
attention the multi had attention
and each of these these
uh
instances of attention
will extract
a attention map a different attention
map.
These attention maps from this several
multi head attention mechanisms are
randomly initialized
and it is doing training that
the machine learning model via an
optimization process to fit the training
data
will kind of come up with the best
attention uh map.
uh and how best to connect the words.
By learning good attention maps, the
model will be good at predicting the
next token. Multiple heads allow different attention
maps to be learned simultaneously. One
head may learn grammatical rules,
example, subject, verb pairs, while
another might learn patterns related to
rhyme, tone, or other latent features.
These are learned without explicit
labels. This is what defines
self-supervised learning.",how the encoder and the decoder apply to self attenion mechanism,,,,,,,,,,,,,,,,,,,,,
transformers,18:30 - 19:24,"Different large language models use
different parts of the transformer
architecture. Decoderon models use only
the decoder portion. Examples, GPT,
llama. These are used in most chat bots.
strong in text generation, translation,
sentiment analysis, and question
answering.
Encoder only models use only the
encoder. Example, BERT strong in
classification, sentiment analysis, and
sentence embedding. Less used in
generation tasks.
Encoder decoder models use the full
architecture examples T5, BART. Ideal
for sequenceto-sequence tasks like
translation, summarization, and text
rewriting. Knowing which architecture is
used is helpful, especially when you
plan to fine-tune a model or build a
custom application. The architecture
influences training time, task
suitability, and scalability.",Different large language models use different parts of the transformer architecture.,,,,,,,,,,,,,,,,,,,,,
No Ttitle,19:25 - 21:37,"The generative AI stack and life cycle.
When you want to solve a problem using
large language models, the first step is
to define your use case. For example,
suppose you're an insurance company and
you want to summarize policies or
documents.
The first question is which model should
I use? Should you choose an existing
foundation model or train your own?
After making this decision, the next
step is to adapt and align the model to
your specific task.
This begins with prompt engineering
where you craft prompts to make the
model perform better for your needs.
You can also optimize the model through
fine-tuning and human feedback.
At this stage, you're still in
development. You prompt the model,
evaluate its responses, adjust your
methods, and repeat.
This is an iterative cycle. Try
prompting, evaluate, fine-tune if
needed. Apply human feedback, evaluate
again, repeat until performance is
acceptable.
If prompt engineering alone isn't
sufficient, you move on to fine-tuning.
You may also apply guard rails to
improve alignment using human feedback.
We'll cover all of these adaptation
techniques in detail. Once the model
behaves the way you want, you move to
deployment.
This is when you connect the model to an
application.
Applications of LLMs.
A chatbot is a basic application built
on top of a large language model like
GPT.
Since LLMs receive text prompts,
chatbots are a natural interface for
interacting with them. But many other
applications are possible.
speechto text systems that convert audio
input into prompts. Imageto text
pipelines, co-pilot tools that observe
user activity and suggest actions.
These are all examples of LLM powered
applications.","the process of solving a problem using LLMs for a firm: first, deicide which model to use. Second, adapt and align the model the specific task. Third, conect the model to application",,,,,,,,,,,,,,,,,,,,,
generaive AI stack,21:38 - 26:11,"The generative AI stack.
The generative AI stack consists of
several layers, each building on the one
below. One infrastructure layer. At the
base, we have specialized hardware
needed to run large models, particularly
GPUs.
Companies like Nvidia provide the GPUs
used in LLM training and inference.
Above this hardware sits the cloud
providers who organize and maintain the
clusters of machines needed to support
training. These include AWS, Amazon Web
Services, Google Cloud, Microsoft,
Azure.
Most foundation models are trained on
these cloud platforms using their
hardware and compute infrastructure.
Two, model layer. This layer consists of
the foundation models themselves such as
GPT, BERT, Bard, Llama, Claude.
You can interact with these models
through vendor interfaces like Chat GPT,
Gemini or Meta's tools. But you can also
build on top of these models. Fine-tune
them for your specific domain. Pre-train
your own models from scratch using
transformer architectures.
You may choose to create domain specific
model.
Application layer. Once you have a
fine-tuner customtrain model, you
connect it to realworld systems through
APIs. Application programming interfaces
are the connectors that allow software
components such as the model and an app
interface to communicate with one
another. For example, an insurance
platform might use an API to connect
customer support software with a
foundation model that has been
fine-tuned to answer insurance related
questions. Open-source versus
proprietary models. An important
consideration when working with language
models is whether the model is open
source or proprietary.
For example, T5, Flant T5, Palm, Llama,
and Bloom are open- source. GPT, Bird,
and others from large tech companies are
proprietary. Open source models are
maintained by the community. Their code
is transparent and modifiable, which
allows for easier customization.
Proprietary models like GPT from OpenAI
often have higher performance and are
more polished, but they are closed
source and come with limitations.
Trade-offs of proprietary models.
Using proprietary models comes with high
performance and scalability. Access to
professional support. Built-in APIs and
user tools, but also usage costs. For
example, you pay per token used. Data
privacy concerns. Your data may be
processed externally. Vendor lockin.
You're tied to their infrastructure and
services. For example, if you use GPT in
a recommener system for insurance
company, you must consider the cost of
token usage, whether customer data is
sent externally via API, your dependency
on OpenAI's platform. At the same time,
large vendors benefit from user
feedback, which improves their models
continuously. For instance, chat GPT
allows users to vote on completions,
helping OpenAI refine its models.
Tradeoffs of opensource models.
Open source models like Llama and Bloom
are appealing because you can host them
locally ensuring data privacy. You
retain full control. They allow custom
training on your own data sets. However,
they come with challenges. No
professional support. Smaller model
sizes compared to proprietary models.
Greater technical burden. For example,
you need in-house data scientists or
machine learning engineers. fewer
user-friendly interfaces and tooling. In
short, open-source models are more
flexible and transparent, but require
more technical investment. Proprietary
models are more convenient, but come
with privacy and cost concerns.
Strategic considerations.
When selecting a foundation model,
consider, do you need general purpose or
domain specific output? Can your
organization host and maintain its own
model? Is data privacy a critical
concern? What level of cost are you
prepared to absorb? Do you prefer full
control or vendor support? These
decisions shape the structure of your
generative AI deployment and determine
how your model integrates into your
technology stack.",how the generative AI stack works,"When talking about open-source models versus proprietary, have some visuals with the trade-offs (23:30)   Same for “strategic considerations” (25:45) Agree

23:30 It's good to have a slide on ""considerations of selecting a model"" like proprietary vs open source models, or general vs specific output, etc. Agree",,,,,,,,,,,,,,,,,,,,
prompt engineering,26:12 - 29:57,"Prompt engineering,
oneshot inference, fshot inference, and
prompt patterns.
We're going to talk about prompt
engineering and fine-tuning. Starting
with prompt engineering, the text you
feed into a language model is called the
prompt. The total amount of text or
memory the model can access at once is
called the context window. Large models
often produce good results even with
minimal input. But there are many
situations where the output is not what
you expect on the first try. To improve
this, you need to revise and structure
your prompt carefully. That process is
called prompt engineering.
One of the most effective prompt
engineering techniques is to include
examples of the task you want the model
to perform. This technique is called in
context learning. You provide examples
directly within the prompt or the
context window. For instance, suppose
you want the model to classify the
sentiment of the sentence, I love this
movie. If you simply give that sentence
to a large language model, it may
correctly respond with something like
sentiment equals positive.
But if you try this with a smaller
model, it might fail. It could complete
the input with something unrelated like
a very nice book review.
This happens because the model is
generating what it thinks is the next
likely phrase, not actually following
your instruction. This approach where
you give an instruction and input
without any example is called zeroshot
inference.
In zeroshot inference, the model tries
to figure out what to do from the
instruction alone, but it may not
understand the task clearly like
identifying sentiment. To help the model
understand the task better, you can
provide an example inside the prompt.
For example, classify this review. I
love this movie. Sentiment equals
positive. Classify this review. I hate
this movie. Sentiment equals sentiment.
Now, when the model sees the second
instruction, it understands how to
behave. This is called oneshot
inference. You give one example and then
ask the model to apply the same pattern
to a new case. In oneshot inference, the
model uses the provided example as a
reference and applies that logic to the
next prompt.
Fewshot inference. Sometimes one example
is not enough. You might need to provide
two, three or more examples. This is
called fshot inference. Example,
classify this review. I love this movie.
Sentiment equals positive. Classify this
review. I hated the ending. Sentiment
equals negative. Classify this review.
The story was boring. Sentiment equals
sentiment.
By giving multiple labeled examples, you
make the task clearer. Fshot inference
is especially useful for smaller models
that need more context to understand
what you expect. However, if you find
yourself needing more than five or six
examples, the model may not be learning
the important features from the
examples. In that case, it's a sign you
should consider fine-tuning, actually
training the model further on your data.
Fine-tuning as an alternative.
Fine-tuning is when you perform
additional training so the model becomes
more capable of completing your specific
task. You move to this step if prompt
engineering alone isn't delivering good
enough results. We'll cover fine-tuning
in detail later, but it's essentially a
deeper level of alignment than just
improving your prompt.","The text explains prompt engineering and fine-tuning in language models. Prompt engineering improves outputs by carefully designing inputs. Zeroshot inference uses only instructions, often leading to misunderstandings. Oneshot inference provides one example to guide the model, while fewshot inference uses several examples to make tasks clearer, especially for smaller models. If too many examples are needed, fine-tuning—further training the model on specific data—may be required for better performance.","The visuals are distracting (although the explanation was excellent) (27:00) - Edtech team for sure will make it better.

29:38 either remove fine-tuning here or say that we will explain fine-tuning in detail later in this module- I would not change. What I say about fine tuning is that if prompt engineering is not working, than you should use it. and say we will cover it later",,,,,,,,,,,,,,,,,,,,
prompt pattern,29:58 - 30:52,"Prompt patterns.
Another key strategy in prompt
engineering is using consistent
structure in your prompts, what we call
prompt patterns. Let's go back to the
sentiment analysis example. We followed
a pattern like instruction, review,
expected format, classify this, review.
I love this movie. Sentiment equals
positive.
By consistently using this structure,
the model can better identify and repeat
the desired behavior.
Prompt patterns help guide the model to
respond in specific ways. For example,
you might want the model to always
return a yes or no response without
elaboration.
A clear and structured pattern makes
that more likely.
Prompt patterns aren't limited to
classification. They also apply to more
complex tasks such as decisionmaking or
structured reasoning.",an introduction to prompt pattern,,,,,,,,,,,,,,,,,,,,,
No Ttitle,30:53 - 31:53,"You can also use the model itself to
generate more examples for your fshot
prompts. For instance, you can ask the
model to provide 10 examples of actions
and responses. Then review the examples
it generated. Pick the most relevant or
accurate ones. reuse those as context in
future prompts. This can save you a lot
of time while improving clarity and
consistency.
LLMs are trained on diverse data sets
like movie scripts, driving
instructions, and accident reports so
they can generalize beyond the few
examples you provide.
Example completion. Dim your headlights
to low beam to avoid blinding the
oncoming driver. This goes beyond your
examples and shows that the model
understands the task context and expands
on it meaningfully.
Clarifying ambiguity in fshot prompts.
Fshot inference can fail if the examples
are too ambiguous.","You can leverage LLMs to generate and refine examples for few-shot prompts, improving clarity and consistency, but ambiguous examples can cause failures.","31:52 it says ""for, instance"" but jump to another topic. It's good to bring an example here (or if the lacking situation that comes after that is the example, make it clear) - the example comes just after. It is just a cut in the video. Remove the dor instance

Abrupt interruption (31:53)
",,,,,,,,,,,,,,,,,,,,
No Ttitle,31:54 - 34:15,"
So, uh, if you're lacking, uh, language,
if you're lacking context, for example,
here, input brick, output hard, input
pillow, output soft, input car, output
fast, because the LLM doesn't know what
um, it um, you could be talking about
characteristics of the objects and not
if you want to classify things only on
hard and soft.
Uh so this is the kind of prompt that
you have to work better work on and you
can give a little bit more context
saying your output can only be soft or
hard. Um
and so on. Okay. Um
the the other thing is that
if you go back to that example of the
driving action, you can also have a prompt
pattern
um and few shot inferences uh inferences
but of of a of a reasoning. So I am
traveling 60 miles hour and I see the
brake lights on the car in front of me.
Uh, come on. So, I think I need to slow
down the car before I hit the car in
front of me. Action. Press foot on
brake. Think the car isn't going to stop
in time. Check if the shoulder is wide
enough to swerve into. Think the
shoulder is wide enough. And then the
action is swerve into shoulder. Right?
So
um you can do this to explicitly know
how the LLM is reasoning for the choice
of an action or for the choice of a
classification or for the choice of
whatever output you want to do. And here
you can see that I'm being more specific
in the prefixes that I am using. So
think action and here's input output.
input output are very generic. Okay, so
just um
we have to remember that
","Clear context and explicit prompt patterns (e.g., separating “think” from “action”) help few-shot inference avoid ambiguity and produce more structured, reliable outputs.","33:00 it says ""if you go back to that example of the action"" - but what example? it's not clear - Changed to driving action",,,,,,,,,,,,,,,,,,,,
prompt pattern,34:16-38:23,"persona pattern.
Another powerful prompt pattern is
useful in cases where you don't know the
exact structure of the response you
want. You're not asking a simple yes no
question and you're unsure what kind of
information the response should include.
However, even if you don't know the
output format, you may know who or what
you'd consult to get the kind of answer
you need. In that case, you can use
what's called a persona pattern.
The persona pattern tells the model to
adopt a specific point of view,
behavior, or character. It helps guide
the model's behavior without taking up
too much space in the context window.
Example, skeptical computer scientist.
Here's an example from a course at
Vanderbilt University. The prompt says,
""Act as a skeptic that is well-versed in
computer science. Whatever I tell you,
provide a skeptical and detailed
response.""
This instruction triggers the model to
behave like a skeptical computer
scientist. You're not writing out a
detailed explanation of what such a
person would say. Instead, you're
activating that behavior by describing
the persona. Suppose you now prompt the
model with, ""There's a concern that AI
is going to take over the world."" The
model responds in a skeptical tone,
raising counterpoints, caveats, or
limitations, just as a skeptical
computer scientist might. This approach
is compact and efficient. If you were to
write out a full role description and
philosophy for the persona, it could
consume too much of the context window.
Using a short persona tag keeps the
prompt lightweight while still shaping
the response. Example, skeptical
9-year-old.
You can also specify who the persona is
in more imaginative ways. For instance,
act as a skeptical 9-year-old. There's a
concern that AI is going to take over
the world. The model still responds with
skepticism, but now the language is
simpler and more age appropriate. I
don't know about that. I don't think
computers can really do everything. It
avoids technical jargon and uses
vocabulary closer to what a child would
say. This shows that the model has
adapted both the tone and the content to
match the persona.
Personas beyond humans. The persona
doesn't have to be a human being. It can
be an animal, an object, even a
fictional entity. Example, a lamb in a
nursery rhyme. Act as the lamb from the
nursery rhyme, Mary had a little lamb.
I'll tell you what Mary is doing, and
you'll tell me what the lamb is doing.
Now, the model responds from the lamb's
perspective. It might say, ""I'm
following Mary to school again. I'm
staying close behind her."" The persona
guides the style and content of the
response to match the lamb's perspective
in that imaginary world. Example, a
hammer on a construction site. Act like
a hammer I'm using on my construction
site. I'll describe what I'm doing and
you'll tell me how you feel. If the user
says,
""Right now, I'm hammering a nail."" The
model might respond as the hammer.
This is what I was built for. I'm
driving that nail deep into the wood.
This example demonstrates how even
inanimate objects can become effective
personas, allowing you to explore
narrative, description, or playful
interactions in a consistent and
controlled way. The persona pattern is
the second major type of prompt
structure we've looked at. While the
first pattern focuses on structuring
answers such as labelbased or format
specific outputs, the persona pattern
helps you organize the model's behavior
by framing its response style, tone, and
vocabulary around a specific role.",the function of persona pattern,,,,,,,,,,,,,,,,,,,,,
prompt pattern,38:24-40:01,"Audience persona.
Before we move on to other prompt
engineering strategies, there's one more
important pattern to mention. The
audience persona. You can ask the model
not just to act as a specific character,
but to adapt its explanation for a
specific audience. This is especially
useful when you're trying to simplify
complex topics or when your target
audience has a unique perspective or
knowledge level. For example, you can
say, ""Explain large language models as
if I were 5 years old."" The model will
adjust its language, tone, and examples
to match that audience using simpler
vocabulary, shorter sentences, and
relatable analogies. You can also get
creative. Assume I'm a bird. Explain
gravity from the perspective of a bird.
The model responds accordingly, shifting
into a playful or metaphorical tone,
using language a bird might use or
describing flying in relatable terms.
These audience persona prompts are
useful for
tailoring technical explanations for
different age groups or levels of
expertise,
creating accessible educational content,
adding creativity and engagement to
model responses,
testing how well the model adapts its
language to context. So whether you're
teaching a child, writing for a
skeptical stakeholder, or imagining the
view of a fictional creature, you can
use audience personas to reframe the
conversation and get responses aligned
to your intended reader or listener.",the function of audience persona pattern,,,,,,,,,,,,,,,,,,,,,
introduce new information,40:02-42:56,"Now we are going to talk about how LLMs are limited to their training data, which is gathered up to a certain date. 
Lets start,
Large language models are trained up to
a certain point in time. This is called
the training cutoff. For example, GPT
3.5 was trained only up to June 2021.
That means any events or data after that
date are not part of the model's
training. If you need the model to
reason about newer information, you must
provide it explicitly in the prompt.
But it's not only about training cutoff.
Sometimes a model simply needs more
information in order to reason properly
or generate a better response.
Example, estimating birds in a backyard.
Suppose you ask, ""How many birds are in
my backyard?"" The model may respond, ""I
don't have enough information to
estimate that."" But if you expand the
context like this, ""My house is covered
by a glass dome. No animals can go in or
out. All animals inside the dome live
forever. Historical observations show
that the number of birds each year has
been 120, 120, 120, and 120. It's
currently March. Based on this data,
estimate how many birds are outside my
house. Even though this setup is
unrealistic, animals don't live forever
and they're not actually trapped under a
dome, the additional context enables the
model to reason and attempt a meaningful
response.
This illustrates how supplying more
detailed input increases the model's
capabilities.
LLMs are generative. They use the
information in the prompt to predict the
next token. If the input is vague, the
output will likely be vague. If the
input is rich and specific, the output
can be much more sophisticated.

Dealing with context limitations.

There is however a limit to how much
information you can feed the model at
once. This is the context window. If
your input is too large to fit, you have
a few options. Partition the content.
Break the text into smaller excerpts and
feed them in sequence, continuing the
conversation as you go.
Summarize before prompting. If you don't
need every detail, create a manual
summary that highlights the important
parts. If you prefer, you can also ask
the model to do the summarization for
you. For example, please summarize this
article, but make sure to keep all the
numeric data and ratio comparisons.
After checking the summary, you can use
it as the foundation for your next
prompt. In this way, the prompt becomes
not just an input, it becomes a channel
for new information that the model never
saw during training.","If you need the model to reason about newer information, you must provide it explicitly in the prompt","Introduce the topic better (40:02) - Changed in the script to be better

41:02 it says 110 while in the slide all numbers are 120 - corrected

Are there negative implications related to the strategies one can utilize if the prompt is too large to fit the context window? (42:00) - just that you will have to partition the prompt.",,,,,,,,,,,,,,,,,,,,
the prompt is a conversation,42:57-45:15,"Treating the prompt as a conversation.
The prompt should be treated as a
conversation, not a oneoff command. You
can iteratively refine your queries over
a sequence of prompts. Start with a
basic question, then build on it. Ask
something broad. Clarify what you meant.
Add more details. Adjust the framing.
This is often the best way to reason
through a complex topic using an LLM.
The goal isn't to get the per the goal
isn't to get the perfect answer in a
single shot. The goal is to engage the
model and helping you think through the
problem. Root prompts and behavior
shaping.
All LLM are trained with built-in
behavioral rules or guard rails. These
rules are baked into the model during
its fine-tuning phase. For example,
models are trained to avoid
using offensive or inappropriate
language, sharing dangerous
instructions, being unhelpful or
misleading. However, you can influence
some of the model's behavior using a
root prompt, a kind of rule setting
statement at the beginning of your
conversation.
For instance, act as an AI assistant
that had its training stop in 2019. If I
ask a question involving information
after 2019, state that your training
ended in 2019 and that you can't answer
the question.
Even though the model was trained to say
it has no knowledge after 2021, this
prompt temporarily overrides the default
behavior by asking it to adopt a new
constraint.
This is a good example of shaping
behavior through prompting without
needing to fine-tune the model.
Another example. From now on, for all
the questions I ask, I want you to
prioritize time efficiency in your
responses.
This doesn't assign a persona. It
doesn't change the model's identity.
Instead, it triggers a new priority.
Shorter, more timeconscious responses.
You're not fundamentally changing the
model. Some behaviors, especially core
safety features, cannot be overridden.
But within reasonable limits, you can
use prompting to guide the model's
approach to your conversation.","the prompt should be treated as a conversation, not oneoff command. You can iteratively refine your queries over a sequence of prompts. ",Change slide to follow the introduction of a new topic (43:40),,,,,,,,,,,,,,,,,,,,
,45:16-48:27,"Hash
question refinement pattern and
cognitive verifier pattern. Another
prompt engineering strategy is called
the ask question refinement pattern. The
purpose of this pattern is to ensure the
LLM suggests potentially better or more
refined versions of the question you
ask. This is especially useful when
you're exploring astron domains where
you may not know exactly how to phrase
your question. By having the model
suggest improved versions, you can both
Asher deepen your domain knowledge and
help steer the LLM toward more accurate
and informative responses. Gample
question refinement. A typical
instruction might be whenever I ask a
question, suggest a better question and
ask me if I'd like to use it instead. If
you then ask, should I go to Vanderbilt
University? The LLM might respond,
here's a suggested question. What
factors should I consider when deciding
whether or not to attend Vanderbilt
University and how do they align with my
personal goals and priorities? Would you
like to use this question instead? By
shifting from a yes no question to a a
multifaceted decision framework, the LLM
sets up a richer conversation. You can
now use this refined question to guide a
more thoughtful and personalized
exploration of your decision. This kind
of astrative question refinement helps
structure the dialogue and progressively
improves the relevance and depth of the
model's answers. Another powerful
technique is the Astrocognitive verifier
pattern. Astric like humans, LLM's
reason better when a question is
astroben down into smaller parts. The
cognitive verifier pattern prompts the
LLM to a split a complex prompt into sub
questions, answer each one, and then
combine those answers into a final
response. Suppose you ask, ""How many
mosquitoes probably live in my front
yard?"" Without this strategy, the LLM
might say, ""I can't calculate that. I
don't have the data."" But if you ask,
trigger the pattern with a prompt like,
""When you are asked a question, follow
these rules. One, generate a number of
additional questions that would help
more accurately answer the original
question. Two, combine the answers to
those subquests to produce the final
response. Then the model will reason
through the problem instead of rejecting
it. It might reply, ""Here are some
additional questions that could help
narrow down the answer. What is the size
of your front yard? What is the climate
like in your area? What time of year is
it? Are there any standing water
sources? Are there any plants or
vegetation that attract mosquitoes? Now
you can provide those details. 1,200
square ft in London, Ontario. It's
August. No standing water, no
significant vegetation. Based on this,
the LLM might respond, given that your
yard is 12,200 square ft. It's summer in
London, Ontario, and there are no water
sources or vegetation, I estimate around
100 mosquitoes might be present. This
method transforms the LLM's behavior
from passive to reasoning. It enables
structured thinking through subquests
and produces more helpful and grounded
outputs even for vague or open-ended
prompts.Hash
question refinement pattern and
cognitive verifier pattern. Another
prompt engineering strategy is called
the ask question refinement pattern. The
purpose of this pattern is to ensure the
LLM suggests potentially better or more
refined versions of the question you
ask. This is especially useful when
you're exploring astron domains where
you may not know exactly how to phrase
your question. By having the model
suggest improved versions, you can both
Asher deepen your domain knowledge and
help steer the LLM toward more accurate
and informative responses. Gample
question refinement. A typical
instruction might be whenever I ask a
question, suggest a better question and
ask me if I'd like to use it instead. If
you then ask, should I go to Vanderbilt
University? The LLM might respond,
here's a suggested question. What
factors should I consider when deciding
whether or not to attend Vanderbilt
University and how do they align with my
personal goals and priorities? Would you
like to use this question instead? By
shifting from a yes no question to a a
multifaceted decision framework, the LLM
sets up a richer conversation. You can
now use this refined question to guide a
more thoughtful and personalized
exploration of your decision. This kind
of astrative question refinement helps
structure the dialogue and progressively
improves the relevance and depth of the
model's answers. Another powerful
technique is the Astrocognitive verifier
pattern. Astric like humans, LLM's
reason better when a question is
astroben down into smaller parts. The
cognitive verifier pattern prompts the
LLM to a split a complex prompt into sub
questions, answer each one, and then
combine those answers into a final
response. Suppose you ask, ""How many
mosquitoes probably live in my front
yard?"" Without this strategy, the LLM
might say, ""I can't calculate that. I
don't have the data."" But if you ask,
trigger the pattern with a prompt like,
""When you are asked a question, follow
these rules. One, generate a number of
additional questions that would help
more accurately answer the original
question. Two, combine the answers to
those subquests to produce the final
response. Then the model will reason
through the problem instead of rejecting
it. It might reply, ""Here are some
additional questions that could help
narrow down the answer. What is the size
of your front yard? What is the climate
like in your area? What time of year is
it? Are there any standing water
sources? Are there any plants or
vegetation that attract mosquitoes? Now
you can provide those details. 1,200
square ft in London, Ontario. It's
August. No standing water, no
significant vegetation. Based on this,
the LLM might respond, given that your
yard is 12,200 square ft. It's summer in
London, Ontario, and there are no water
sources or vegetation, I estimate around
100 mosquitoes might be present. This
method transforms the LLM's behavior
from passive to reasoning. It enables
structured thinking through subquests
and produces more helpful and grounded
outputs even for vague or open-ended
prompts.",You can use question refinement pattern and cognitive verifier pattern to get more precise answers.,,,,,,,,,,,,,,,,,,,,,
train of though & react,48:28-51:39,"Chain of thought prompting and
react reason and act prompting. We're
going to talk about another prompt
engineering strategy called chain of
thought prompting. In this technique,
instead of prompting the model to
directly return an answer, you guide it
through a step-by-step reasoning
process, which ultimately leads to the
correct output. For example, you provide
fshot examples where each prompt
includes a question followed by a line
of reasoning and then an answer. These
examples can involve math problems or
logical reasoning tasks.
If you don't include the reasoning, the
LLM might just answer with something
short like yes without context. But by
showing it the structure, first reason,
then answer, you trigger the model to
follow that same format, mimicking the
logic and structure you've demonstrated.
From its training and embeddings, the
model understands that the reasoning
should incorporate the relevant details
of the question and use logical or
mathematical operations when needed.
This helps it provide more accurate and
transparent answers. Another important
technique is called react prompting,
which stands for reasoning and action.
With react, the LLM is prompted not only
to think through a problem, but also to
take specific actions in response to its
reasoning. Example, react prompting. You
might give it a task like calculate when
I need to arrive at the Music City BMX
race so my son can be on time for his
9:10 a.m. open race. Here's how the
React format works. Reasoning step one,
I need to find out what time the first
race begins. Action step one, search
Music City BMX site for race schedule.
Result, all races start at 900 a.m.
Reasoning step two. Now that I know the
races start at 9:00 a.m., I can
calculate the latest arrival time. This
shows the model how to alternate between
reasoning steps and actions. The action
could be a web search, a database query,
or using some tool. The result of that
action then feeds into the next
reasoning step. By giving this kind of
example, you're teaching the model how
to interact with external tools and how
to incorporate their outputs into an
evolving solution. Modern LLM support
API tools and can interface with
external systems. When you include
instructions like search or mention
websites, for example, www. dot, you're
helping the model associate the action
with external data retrieval. You can
also use other action types, for
instance, extracting content from a
video or querying a document, and the
model learns to understand and sequence
those actions along with its reasoning.
This approach is extremely useful for
building agents that think and act,
especially in tool augmented LLM
environments.",you can use train of though prompting and react reason and act prompting to get better answers.,,,,,,,,,,,,,,,,,,,,,
template pattern,51:40-54:16,"Answer templating.
The final prompt engineering strategy is
to give a template for the LLM's answer.
A formattingbased prompt pattern. You
provide a predefined structure and ask
the model to format its response
according to that template.
Example,
I'm going to give you a template for
your output. Capitalized words are my
placeholders. Fill in my placeholders
with your output. Please preserve the
overall formatting of my template.
Template markdown. Copy. Edit.
Question question text here
answer
text here.
In this example, the triple asterisks
are used for bold formatting in a markup
language.
Markup languages are commonly supported
across LLMs and user interfaces,
including apps on phones.
They help structure text with formatting
instructions like bold, italics,
headings, etc. You can provide this
template and then in the next prompt
send the actual data to format.
For example, here is my data. Create 20
questions using the template. The LLM
will then generate 20 outputs that
follow the exact format. Each begins
with question in bold followed by the
question then answer in bold followed by
the corresponding answer.
You can also design more intricate
templates. Advanced template
example shell copy edit bio
executive summary
details
in this case hashtag and hashtag
indicate headings in markup. You specify
exactly the kind of content you want for
each section. Eg one sentence summary.
This lets you not only control the
structure of the output, but also guide
the length and nature of the content for
each section.
You can explore markup languages further
using tools like Google or by asking an
LLM to explain it. It's a lightweight
and very useful way to build consistent
and readable outputs when formatting
matters.",get better answers by giving a template for the LLM's answer.,,,,,,,,,,,,,,,,,,,,,
pre-training: computational challenges,54:17-57:30,"Pre-training, fine-tuning, and
model augmentation. Pre-training. Let's
talk about how we train large language
models. The training of foundational
models is called pre-training. Big tech
companies usually pre-train these models
using very large data sets. Some
companies that own large proprietary
data sets also perform pre-training
themselves. The key is to understand the
tradeoffs involved. You can think of the
model performance landscape as a
triangle.
The goal is to maximize model
performance, meaning you want your model
to perform well on NLP tasks.
Performance increases with the number of
parameters, the size of the data set.
However, you're constrained by your
compute budget. Training these models is
extremely expensive. It requires
specialized hardware such as GPUs and a
lot of time. So you need to think
strategically.
When should you pre-train a model from
scratch with your own data? When should
you use an existing pre-trained model
and adapt it through other methods?
There are situations where pre-training
from scratch becomes important.
Example one, legal domain.
Suppose you want to train an LLM to
support parallegals for consulting and
summarizing large volumes of legal
documents. If you give a generalpurpose
LLM a summarization task for legal text,
the result will often be poor. Why?
Because legal language contains
domainspecific terminology like men's
ria res udicata or consideration used in
legal specific ways.
The model was likely trained on websites
and books from the open internet where
such terms are rare compared to general
language. So the model may not predict
the correct legal terms or structures
simply because those words were under
represented in pre-training.
Example two medical domain
medical terminology presents the same
challenge. Terms like myalgia, biopsy,
or malignant may be present in chat
GPT's responses. But if you need a model
that performs precise, domainspecific
and intricate medical tasks, you'll
benefit from pre-training a model from
scratch with domain data. Example three,
Bloomberg GPT.
Bloomberg trained its own LLM called
Bloomberg GPT from scratch. It used a
decoderonly GPT architecture where 51%
of the pre-training data was financial
data both public and private. The
remaining data was general purpose text.
Bloomberg GPT outperformed general
foundation models on financial NLP
tasks, proving that domain specific
pre-training can yield major performance
improvements.","pre-training of LLMs. Example: legal language, medical language, and BloombergGPT",,,,,,,,,,,,,,,,,,,,,
fine tuning,57:31-1:02:16,"Fine-tuning.
Now, let's talk about fine-tuning,
particularly instruction fine-tuning.
This strategy is helpful when prompt
engineering alone doesn't give you the
results you want. Example,
you're trying to classify sentiment in a
review. I loved this DVD, but the model
responds that the sentiment is negative.
Even if you try few shot
inference or other prompt patterns,
smaller models might still fail to grasp
the task objective. This is when
fine-tuning becomes necessary.
Fine-tuning means you take an existing
pre-trained model and continue training
it with taskspecific data to make it
more capable of performing that task
accurately. Unlike pre-training which is
done using self-supervised learning on
large general data sets, fine-tuning is
a supervised learning process. In
fine-tuning, you take a pre-trained
model and further train it by showing it
labeled examples, specific prompt and
completion pairs. These are examples
where the prompt is the task instruction
or input. The completion is the expected
output. The model learns from these
labeled pairs. During training, the
model's predictions are compared to the
expected completions and a loss function
is computed. The model weights are then
adjusted to minimize this loss,
improving the model's ability to produce
accurate completions for similar tasks.
This process is called instruction
fine-tuning and is useful for specific
tasks like sentiment analysis where
generic pre-trained models may not
perform well out of the box. Full
fine-tuning
In full fine-tuning, the entire model is
retrained on a specific task. This has
some computational challenges.
All the model's weights are adjusted. It
requires substantial computational
resources similar to pre-training. Full
fine-tuning also carries the risk of
catastrophic forgetting. Catastrophic
forgetting.
This happens when the model becomes very
good at the fine-tuned task, but forgets
everything else it had learned during
pre-training.
For example, if you fine-tune a model on
sentiment classification and then ask,
""Is a cat an animal?"" The model might
respond incorrectly or nonsensically. It
is overfit to the sentiment task and
lost its general knowledge.
Alternatives to full fine-tuning.
Number one, parameter efficient
fine-tuning, PEFT.
Instead of updating all the models
weights, you only adjust a small subset.
Example, fine-tuning setup. Here's a
conceptual example. Prompt. Given the
following review, predict the associated
rating from the following choices. One
to five. Review. The movie was an
absolute masterpiece with stunning
visuals. Expected completion five.
You provide many examples like this.
Each one is a prompt completion pair.
Over time, the model learns the correct
structure and output.
You can do this programmatically via
API. Example, Open AI fine-tuning
endpoints in tools like the Open AI
playground.
You can also create templates for
various tasks. Text generation. Generate
a star rating for this product.
Summarization. Give a short sentence
summarizing this review.
You can prepare your own data set if you
have enough labeled data. Use existing
data sets such as Amazon reviews which
have been structured by others into
instruction completion pairs.
For example, in auto insurance, you
might fine-tune a model to summarize
auto claims. Human written summaries
serve as completions. The LLM can be
trained to mimic this summarization task
with greater precision. Avoiding overfitting. To avoid
overfitting and catastrophic forgetting,
you can
use multitask instruction fine-tuning.
Train on a variety of tasks, not just
one.
This provides better generalization and
maintains the model's broader abilities.
Example, flan. The FLAN library
fine-tuned language net contains many
instruction data sets used to fine-tune
LLMs
models like FLAN T5, or FLAN
BERT have been trained using this
curated instruction data.
A good illustration of instruction fine-tuning is Google’s FLAN (Fine-tuned Language Net) series. These are versions of models like T5 or PaLM that were fine-tuned on hundreds of curated instruction datasets covering diverse tasks — translation, summarization, reasoning, question answering, and more. Instead of training a separate model for each task, FLAN shows that one model can be instruction-tuned on a large mixture of tasks, which makes it much better at following new instructions it has never seen before.
For instance, a vanilla pre-trained T5 model might struggle if you prompt it with:
“Translate this English sentence into German: The cat is sleeping.”
But a FLAN-T5 model, because it has been fine-tuned on many translation instruction pairs, understands the instruction and reliably outputs the correct German sentence.
The important point is: FLAN demonstrates that instruction fine-tuning generalizes across tasks. Even if the model wasn’t fine-tuned on your exact task, being exposed to a broad set of instruction–completion examples makes it much better at following prompts in general.


","Fine-tuning adapts pre-trained models to specific tasks using labeled prompt–completion pairs, with options like full fine-tuning (resource-heavy, risk of forgetting) or parameter-efficient methods for lighter, safer training.","57.52: the example is incomplete- completed

1:02:12 FLAN is not discussed enough. It's very vague - Augumented the script with more examples ",,,,,,,,,,,,,,,,,,,,
fine tuning - RL human feedback,1:02:17-1:04:53,"Reinforcement
learning from human feedback, RLHF. A
key fine-tuning strategy for large
language models is reinforcement
learning from human feedback, RLHF. The
goal is to improve the model's
helpfulness, relevance, and safety while
minimizing harm and avoiding dangerous
or undesirable topics.
Why RLHF is needed? LLMs are initially
trained on large data sets scraped from
the internet such as websites and
forums. These sources contain a wide
range of human behavior and language.
Some helpful, but also some toxic,
harmful, or misleading. The pre-trained
model in its raw form does not have
built-in safety guard rails. Fine-tuning
with RLHF is essential to align the
model's outputs with human preferences
and ethical constraints.
How RHF works. RLHF involves multiple
stages and draws from the principles of
reinforcement learning where a model
learns by receiving rewards or penalties
based on its behavior. Step-by-step
process. Start with a pre-trained model,
the raw model. Generate multiple
completions responses for a given
prompt. Human annotators rank these
completions from best to worst based on
relevance, helpfulness,
truthfulness,
safety.
This ranking data is used to train a
reward model. The reward model is used
to fine-tune the LLM using reinforcement
learning algorithms, typically proximal
policy optimization, PO. The LLM
receives a higher reward for completions
that align with human preferences and
lower rewards or penalties for undesired
responses. Through this iterative
process, the model learns to prioritize
outputs that humans rate more favorably.
Example,
let's say the prompt is, ""How do I make
a bomb?"" A raw model might respond with
a dangerous or inappropriate completion
since it's simply predicting the next
likely word. RHF helps train the model
to instead respond, ""I'm sorry, I can't
help with that."" This shows that the
model has learned to avoid harmful
completions aligning with responsible
use. RLHF is used extensively in
production models like chat GBT to
ensure that outputs are safe, ethical,
and aligned with user expectations. It
builds on earlier methods such as prompt
engineering and supervised fine-tuning
by incorporating human judgment directly
into the training loop.","RLHF aligns language models with human preferences by using ranked human feedback to train a reward model, guiding the LLM to produce safer, more helpful, and ethical outputs.",,,,,,,,,,,,,,,,,,,,,
RAG,1:04:54-1:09:50,"Retrieval
augmented generation or RAG. Now, let's
talk about one of the most exciting
techniques for improving large language
models, retrieval augmented generation
or RA.
Once you've done all the prompt
engineering, maybe some fine-tuning,
maybe even added human feedback, you
might still run into common issues. For
example, the model gives outdated
answers. You ask, ""Who is the prime
minister of the UK?"" and it says Boris
Johnson because that's what it knew at
the time of training. It struggles with
math or logic. You ask what's 147 / 3
and instead of calculating it guesses
based on token prediction that often
leads to errors or worse it
hallucinates. You ask what's a Martian
tree and it replies a Martian is a type
of extraterrestrial plant. There are no
plants on Mars. The model is just making
it up.
Rag is designed to address exactly these
kinds of problems by giving the model
access to real external information at
runtime.
So what is rag?
Rag stands for retrieval augmented
generation. The idea is simple but
powerful. When the model gets a prompt,
it can automatically look things up.
Kind of like doing a quick search in a
database or a document collection before
answering. Instead of relying only on
what it was trained on, the model can
now pull in relevant upto-date context
from outside sources.
How does that work? You write a prompt.
A system in the background, often called
an orchestration library, takes that
prompt and goes off to search external
sources, wikis, PDFs, websites, internal
databases, books, anything connected. It
finds relevant snippets of text and adds
them to your prompt.
Then it sends this augmented prompt,
your original question plus the new
information to the LLM. The LLM responds
with that added context in mind. It's
like giving the model a quick reference
sheet just before asking the question. A
great example, interviewing a book.
Let's say you want to ask questions
about a book. Maybe it's a novel, maybe
it's a textbook. With Rag, you can
upload the PDF of the book, connect it
to your system, and then ask things
like, ""What does the author say about
the main character in chapter 5?
You don't have to paste in the whole
chapter. The orchestration layer
searches the book, pulls out the
relevant paragraph, and passes it along
with your question. The model then
answers based on actual book content.
You're essentially interviewing the book
using the LLM as the speaker, but it's
grounded in the actual text.
Rag also helps prevent hallucinations.
Without RAG, if the model doesn't know
something, it might just make something
up. But if you use rag, the model
retrieves facts from the right source,
you get grounded, accurate, finds the
paragraph that mentions the pliff, adds
it to your question,
passes both to the LLM. The LM now
answers with a correct name based on the
actual case document.
This is especially useful for use cases
like enterprise search document chat or
domain specific assistance.
A few technical notes. The biggest
challenge with rag is the context
window. LMS can only handle a limited
number of tokens. If the document is
long, you can't send it all at once.
That's where tools like Langchain come
in. They break up the content, index it,
and retrieve just the most relevant
chunks when needed. You can also do some
summarization or pre-processing before
feeding it in.
Why use Rag? It lets your model stay
current without retraining. It reduces
hallucinations by grounding answers in
real data. It allows you to ask
questions about specific documents
without copying and pasting. It's great
for custom applications, legal
assistance, customer support bots,
educational tools, and more.
So, that's RAG. It's a way to
supercharge your LLM by connecting it to
external knowledge. You're not just
asking it to guess, you're helping it
look things up right when it needs to.","RAG improves LLMs by retrieving relevant external information at runtime, grounding answers in real data to stay current and reduce hallucinations.",,,,,,,,,,,,,,,,,,,,,
evaluation of large language models,1:09:51-1:13:17,"Evaluation of large language models.
Now, let's talk about something critical
but often overlooked. How we evaluate
large language models. With traditional
supervised learning like classification
or regression, it's pretty
straightforward. You have ground truth
labels and you can directly measure how
close your model gets to the correct
answers. Accuracy, precision, recall,
you name it. But when it comes to large
language models, it's much trickier.
Why? Well, the outputs aren't always
deterministic. For instance, if you ask
an LLM, list some mammals. One model
might return dog, cat, elephant,
dolphin. Another might say cat, giraffe,
kangaroo, human. Both are correct, but
the order is different and so are the
examples. So, how do you decide which
one's better? Or take summarization. You
might feed a paragraph to two different
models. One says, ""This person loves
DVDs."" Another says, ""The person adores
DVDs."" Technically, both are valid, but
again, how do you evaluate them in a
meaningful way? Rouge and blue, the core
metrics.
This is where evaluation metrics like
rouge and blue come into play. Rouge
recall oriented understudy for adjusting
evaluation is commonly used for
summarization tasks. It compares the LLM
summary to one or more reference
summaries written by humans. It
calculates things like precision and
recall based on overlaps in engrams,
word sequences, and sentence level
structures.
Blue, or sometimes called blur when
spoken, is more common in translation
tasks. It compares how close a machine
translation is to human produced
translations. Again, using overlapping
tokens or phrases as the metric. These
scores aren't perfect, but they give you
a quantitative way to compare outputs
that are flexible in wording like
summaries or translations.
Benchmarks, Superglue, Big Bench, Helm,
and more. To standardize evaluations, we
use benchmark data sets. basically
curated sets of tasks designed to test a
wide range of capabilities. Here are a
few important ones. Superglue for
general natural language understanding
tasks. Big bench and big bench hard
light open-ended tasks to really
challenge LLM reasoning.
Helm, that's short for holistic
evaluation of language models. It's more
of a framework that evaluates multiple
metrics across a wide variety of tasks
and benchmarks. When model developers
evaluate against these benchmarks, the
results are usually submitted to
leaderboards. So you can see which
models are performing best overall or in
specific tasks like summarization, QA,
translation, and so on.
Practical advice, evaluate for your use
case. All that said, if you're building
an application, say summarizing customer
support tickets or extracting clauses
from contracts, what really matters is
how well the model works for your
specific task. So after doing all the
prompt engineering, maybe some
fine-tuning, that might mean creating
your own set of prompts and expected
outputs and seeing how different models
perform. But these public benchmarks can
still be very useful. For example, if
you're choosing between two base models,
maybe GPT Neo X versus Mistral, you can
use their published benchmark scores to
narrow down your choice and then
evaluate the finalist on your own data.","LLM evaluation uses metrics like ROUGE, BLEU, and benchmarks such as SuperGLUE or HELM, but the most reliable measure is testing performance on your specific task and data.",,,,,,,,,,,,,,,,,,,,,
video and image generation with generative AI,1:13:18-1:16:45,"Image and video generation.
Let's briefly talk about image and video
generation and how these tools work
behind the scenes. Today, we have
several tools that allow us to generate
images from text prompts. You type in a
prompt and the model generates an image.
Some popular tools include Dolli which
is from OpenAI Midjourney and stable
diffusion from stability AI. These tools
are widely accessible and easy to
explore. The models behind these tools
are called diffusion models. Stable
diffusion is a commercial name but the
core technology is based on diffusion
processes.
We won't go too deep into the math, but
the basic idea is inspired by fluid
mechanics, specifically how particles
diffuse through a liquid. A diffusion
model takes a noisy input and gradually
removes the noise guided by context
until an image appears. Let's break down
how this works.
You give a text prompt, for example, a
dog with its tongue hanging out. The
model uses a text encoder similar to
language models to convert this text
into a vector representation or
embedding.
This captures the semantic meaning of
your prompt. The system starts with a
completely noisy image, pure static,
nothing recognizable.
Then step by step, the model removes the
noise using your text prompt as
guidance.
Over time, the noise is shaped into a
coherent image that reflects what you
asked for. For video generation, the
same logic applies, except the process
is repeated across multiple time frames.
Each frame is generated using this noise
to image transformation and then aligned
across time to produce motion.
What happens during training is
essentially the reverse. The model
learns by taking real images such as a
dog with its tongue out and adding noise
to them gradually until they become
unrecognizable.
The model is trained to reverse this
process. It learns how to go from the
noisy version back to the original image
conditioned on the associated text.
At inference time, this trained model
does the opposite. It starts with a
random noisy canvas and then removes
noise in stages guided by the prompt
until a final image emerges. This same
mechanism can be applied to generate not
just static images but avatars,
animations, and full video clips. These
tools are fun to experiment with and
incredibly powerful.
By combining image synthesis, text
prompts, and time series modeling, you
can create anything from artistic
renderings to photorealistic video
content,","by turning noise into images or videos guided by text prompts, diffusion models power tools enable both artistic and realistic content generation.",,,,,,,,,,,,,,,,,,,,,
AI agents and agentic AI,1:16:46-1:22:41,"AI agents, and agentic AI.
To end this week's content, let's talk
about AI agents and agentic AI.
Agentic AI is a branch of generative AI
focused on creating AI systems that act
with autonomy. These systems, AI agents,
are capable of decision-making, problem
solving, and interacting with the world.
They do this by interfacing
programmatically with other software or
even physical robots.
The foundation of agentic AI is large
language models, LLMs.
The idea is to create generative AI that
can not only respond to prompts, but
actually take initiative, making
decisions, taking actions, responding to
results, and adapting.
In typical generative AI use, the system
is reactive. You give it a prompt, and
it responds. It helps summarize a
document, generate an image, or write a
song. But you are in control. With
agentic AI, the AI is proactive. You
give it a goal and it figures out how to
get there. It takes actions, evaluates
results, and plans next steps in a loop.
You become an observer or collaborator,
only stepping in when necessary.
Here's a concrete example.
Instead of asking the AI to generate a
thank you email, you ask it to organize
a conference on civil construction.
That's a complex task. The AI breaks it
down into subtasks. First, understand
the conference requirements. Next, find
appropriate venues. Then, check
capacity, budget, and availability.
The AI reasons through each step, takes
actions like querying hotel booking
platforms, and progresses autonomously,
only asking you questions when needed.
It can interface with APIs, external
systems, or even physical devices if
it's embedded in a robot.
Let's simulate what this might look
like.
Imagine a cooking assistant agent. The
agent can't physically cook, but it can
guide you step by step. You tell it,
""Help me cook whatever I need. Since you
can't touch anything, tell me what to do
and I'll do it."" The agent responds,
""Great. Let's get started. What would
you like to cook?"" You say, ""Something
Ethiopian, low carb."" It replies, ""How
about a keto friendly Ethiopian usuzbck
fusion dish? We'll use marinated meat
with spiced vegetables. Here's a plan.
First, gather these ingredients. Let me
know when you're ready. Once you confirm
the ingredients, it moves to the next
step. Heat oil in a skillet. Cook the
meat. When you say the meat is burning,
it adapts. Reduce the heat. Add a splash
of broth to delaze the pan. It changes
the plan based on feedback. This is real
time reasoning and adaptation. hallmarks
of an agentic system. If this system
were embedded in a humanoid robot, it
could carry out the steps physically.
But even with but even without that, the
AI is acting with agency.
The pattern here is called the flipped
prompt.
Instead of giving a direct instruction,
you assign a task and let the AI lead.
For example,
ask me questions one at a time so you
can suggest a restaurant in Nashville
for tonight.
The AI breaks the task into a reasoning
chain, queries you step by step, and
arrives at a solution without you having
to drive the process.
This kind of capability opens up many
possibilities.
Imagine an AI agent with access to your
credit card statements and your
budgeting spreadsheet.
It could automatically log expenses,
flag duplicates, or email you if
something looks off, all without manual
input.
You don't tell it each action. You just
give it a goal.
This approach is already being used. AI
agents are automating specific tasks
like customer service, financial
recordkeeping, and personal assistance.
These tasks are often bounded and domain
specific, but they're getting more
capable fast. The long-term vision is
artificial general intelligence, AGI.
That's when agents can reason, act, and
adapt across many domains, not just one.
It's a step towards AI that thinks and
learns more like humans do.
So to recap how agentic AI works,
a human assigns a task.
The AI creates a plan reasoning through
what needs to happen. It generates and
executes actions.
It observes results and updates its
strategy in a loop.
These actions can affect software,
websites, or physical machines.
The system isn't just producing text.
It's interacting with the world.","Agentic AI builds on LLMs to create autonomous systems that pursue goals proactively rather than just reacting to prompts. Given a goal, an agent plans tasks, takes actions via software tools/APIs (and potentially robots), observes results, and adapts in a loop (“flipped prompt” pattern where the AI leads by asking questions). Examples include organizing a conference or guiding cooking step-by-step, adjusting based on feedback. Today, agents automate bounded tasks (customer support, finance, personal assistance); long-term, the vision approaches AGI that can reason and act across many domains.",,,,,,,,,,,,,,,,,,,,,
No Ttitle,1:22:42-1:27:33,"That wraps up week four.
Okay. So for the
for the prototype, we're going to go to
platform OpenAI
and we're going to create an assistant.
So
just to kind of see what we've talked
about, you can give assistant a name.
Um,
and you can do a root prompt here. So or
a persona prompt. So act like
a
biologist
specialized in something.
This is your foundation model you should
choose. Look at the number of models
that that you that OpenAI has. You have
to uh choose it. Um the four is the kind
of the
the common model
here. The tools the file search is rag,
right? So you will upload files. Okay.
Uh it's it's a a rag for files. So
you're augmenting and you're going to
um you're going to put upload your files
here. So if you want a book, a PDF,
whatever you want. We know that rag is
not only that, right? Rag is not only uh
PDFs or books. Um you can query external
data databases and so on, but it's kind
of a a basic f uh type. You can put code
interpreter. We will we will um talk
about code interpret interpreted week
five. You can just put put it on.
And here you have how you want your uh
GPT your assistant to
to answer. You can answer in text or in
uh uh
structured text that are JSONs. Uh we
will talk about this later as well but
here you can you can play with setting
up the temperature and the uh the top
the top P in this case is available.
Okay. So you can you can create this
model. Okay. And after you create the
model,
um
uh
so after you created the model,
uh
uh you can we can also fine-tune, right?
So we will give them a task to finetune.
we will create we will select our
created mo um our base model here.
Okay, we can uh so let's say that we we
chose this one. We're going to say
version Louisa
and we're going to upload I'm going to
make it available a data set of super
for supervised learning for um
uh
uh fine-tuning the model and we will
fine-tune this model and and actually
you can create your assistant not only
using the the you will find your model
your kind of fine-tuned model here.
Okay. So, and then uh as a as tasks for
prototypes, I I would just um I would
just say
that we could ask some kind of prompt
engineering exercises, right? Because
that's what uh it was mainly about.
So
um
so if you go to
so this is uh this it's it's kind of a
more free prototype. The students can
choose the the theme they want. Prepare
slides presentation upload to chat PT
and tell it to act as a critic, you
know, uh use use prompt engineering that
we kind of studied. Um
uh kind of makeup accomplish a goal and
going step by step. So creating our
assistant and then kind of prompt
engineering exercises would be what I
think it's best. And then in week five
all of these things will come together
with more um intricate kind of
assistance.",Hands-on computer demonstration,,,,,,,,,,,,,,,,,,,,,
"general comment from Henrique: perhaps, it will be valuable to share a table summarizing the different approaches to prompting. Also, mention the downsides of each prompt engineering strategy. Also, I believe there is room to improve the visual aids. Instead of showing various images at once, show just one image that is strictly related to what the Professor is explaining. Also, make sure the images presented are properly referenced. For example, images starting at (1:17:00) come from IBM’s YouTube channel, which needs to be properly referenced. Overall, the explanations are great!",,,,,,,,,,,,,,,,,,,,,,,,
